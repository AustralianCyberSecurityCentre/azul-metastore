"""Base class for ingestors."""

from __future__ import annotations

import logging
import time
import traceback
from typing import Any

from azul_bedrock import dispatcher as b_dispatcher
from azul_bedrock import exceptions as azbe
from azul_bedrock import models_api as azapi
from azul_bedrock import models_network as azm

from azul_metastore import context, settings
from azul_metastore.query import binary_create, plugin, status

logger = logging.getLogger(__name__)


class DataException(Exception):
    """Error getting data from dispatcher."""

    pass


class BaseIngestor:
    """Manage ingestion loop for topics from dispatcher to be processed."""

    model: str
    max_count: int = 100
    # changing the version will re-consume events from kafka
    version = "2025_05_19"
    processor_type = "ingestor"

    def __init__(self, ctx: context.Context):
        """Init."""
        self.s = settings.get()
        self.url = self.s.dispatcher_events_url
        if not self.model:
            raise Exception(f"event type not set for {self.__class__.__name__}")

        self.ctx = ctx

        # name includes partition in cases where two ingestors are running with different partitions.
        self.dispatcher: b_dispatcher.DispatcherAPI = b_dispatcher.DispatcherAPI(
            events_url=self.s.dispatcher_events_url,
            data_url=self.s.dispatcher_streams_url,
            retry_count=3,
            timeout=60,
            author_name=f"{self.processor_type}-{self.s.partition}",
            author_version=f"{self.version}-{self.s.ingestor_version_suffix}",
            deployment_key="",
        )

    def _get_data(self) -> tuple[azapi.GetEventsInfo, list[Any]]:
        raise NotImplementedError()

    def get_data(self) -> list[Any]:
        """Read documents from dispatcher."""
        try:
            _, events = self._get_data()
        except azbe.DispatcherApiException as e:
            if e.status_code != 200:
                content_slice = None
                if e.response is not None:
                    content_slice = e.response.content[:1000]
                raise DataException(f"bad status {e.status_code}\n{content_slice}") from e
            raise DataException("connection error") from e
        return events

    def is_done(self) -> bool:
        """Return true if data read is complete."""
        return False

    def set_data(self, docs: list[Any]) -> None:
        """Save data to opensearch."""
        raise NotImplementedError()

    def main(self):
        """Perform ingestion loop."""
        logger.info(f"start ingestor loop for {self.model} events")

        last_print = time.time()
        slept_printed = False
        first = True
        count = 0
        time_spent_getting_data = 0
        time_spent_setting_data = 0
        while not self.is_done():
            logger.debug(f"fetch {self.model} events")
            try:
                start_get_data = time.time()
                d = self.get_data()
                time_spent_getting_data += time.time() - start_get_data
            except DataException:
                traceback.print_exc()
                logger.error("dispatcher fetch failure")
                time.sleep(10)
                continue

            if not d:
                if not slept_printed:
                    slept_printed = True
                    logger.info(f"no {self.model} events available, sleeping")
                time.sleep(10)
                continue

            logger.debug(f"write {len(d)} {self.model} events")
            start_set_data = time.time()
            self.set_data(d)
            time_spent_setting_data += time.time() - start_set_data

            count += len(d)
            now = time.time()
            if last_print < (now - 10) or first:
                logger.info(
                    f"wrote {count:05d} {self.model} events in {(now - last_print):.2f}s "
                    + f"(Get time was {time_spent_getting_data:.2f}s, Set time was {time_spent_setting_data:.2f}s)"
                )
                time_spent_getting_data = 0
                time_spent_setting_data = 0
                slept_printed = False
                first = False
                last_print = now
                count = 0

        logger.info(f"no more {self.model} events")


class BinaryIngestor(BaseIngestor):
    """Binary events to metastore."""

    model = "binary"

    def _prefilter(self, results: list[azm.BinaryEvent]) -> list[azm.BinaryEvent]:
        """Ensure events have ID generated by dispatcher."""
        to_process = []
        for ev in results:
            if not ev.kafka_key:
                logger.info(f"kafka_key not set, skipping:\n{ev.model_dump_json()}")
                continue
            to_process.append(ev)
        return to_process

    def _get_data(self):
        return self.dispatcher.get_binary_events(
            count=self.max_count,
            deadline=30,
            # read expedite events so that high priority binaries have results in opensearch quicker
            require_expedite=True,
            # read historic events so that changing the opensearch prefix will reload all events from kafka
            # do not want to overlap live and historic as that would 2x load on opensearch
            require_historic=True,
        )

    def set_data(self, results: list[azm.BinaryEvent]) -> None:
        """Write docs continually, logging errors."""
        results = self._prefilter(results)
        binary_create.create_binary_events(self.ctx, results)


class PluginIngestor(BaseIngestor):
    """Plugin registration events to metastore."""

    model = "plugin"

    def _get_data(self):
        # read historic events so that changing the opensearch prefix will reload all events from kafka
        # do not want to overlap live and historic as that would 2x load on opensearch
        return self.dispatcher.get_plugin_events(
            count=self.max_count,
            deadline=30,
            require_historic=True,
        )

    def set_data(self, results: list[azm.PluginEvent]) -> None:
        """Write docs continually, logging errors."""
        plugin.create_plugin(self.ctx, results)


class StatusIngestor(BaseIngestor):
    """Plugin produce/optout events to metastore."""

    model = "status"

    def _get_data(self):
        # read historic events so that changing the opensearch prefix will reload all events from kafka
        # do not want to overlap live and historic as that would 2x load on opensearch
        return self.dispatcher.get_status_events(
            count=self.max_count,
            deadline=30,
            require_historic=True,
        )

    def set_data(self, results: list[azm.StatusEvent]) -> None:
        """Write docs continually, logging errors."""
        status.create_status(self.ctx, results)
