"""Ensure binary2 indices are internally consistent."""

from typing import Generator, Iterable

from azul_metastore import context
from azul_metastore.common import utils


def get_dispatcher_datastreams(ctx: context.Context, sha256: str) -> Generator[tuple[str, str, str], None, None]:
    """Return source, label and sha256 for every datastream associated with the binary."""
    body = {
        "query": {
            "bool": {
                "filter": [
                    {"ids": {"values": [sha256]}},
                ],
            }
        },
        "aggs": {
            "CHILDREN": {
                "children": {"type": "metadata"},
                "aggs": {
                    "SOURCES": {"terms": {"field": "source.name"}},
                    "DATASTREAMS": {
                        "terms": {"field": "uniq_data"},
                        "aggs": {
                            "DATASTREAMS": {
                                "top_hits": {
                                    "size": 1,
                                    "_source": [
                                        "datastreams.label",
                                        "datastreams.sha256",
                                    ],
                                }
                            }
                        },
                    },
                },
            }
        },
    }
    resp = ctx.man.binary2.w.search(ctx.sd, body)
    aggs = resp["aggregations"]["CHILDREN"]
    sources = [x["key"] for x in aggs["SOURCES"]["buckets"]]
    datastreams = []
    if len(aggs["DATASTREAMS"]["buckets"]) > 0:
        datastreams = [
            (x["label"], x["sha256"])
            for x in aggs["DATASTREAMS"]["buckets"][0]["DATASTREAMS"]["hits"]["hits"][0]["_source"]["datastreams"]
        ]

    # yield every combination of source and sha256 for dispatcher object store operations
    return [(source, label, sha256) for source in sources for (label, sha256) in datastreams]


def ensure_valid_binaries(ctx: context.Context) -> int:
    """Ensure that all binaries have associated submissions.

    * All 'binary' events with no submission 'metadata' are deleted.
    * All 'metadata' events with no 'binary' are deleted.

    This doesn't fix logical connections between different binaries such as parent-child relationships.
    """
    # ensure that next delete factors in any deletion we just performed
    ctx.refresh()

    # delete binary docs with no submission
    # this indicates there is no valid source for the binary
    body = {
        "query": {
            "bool": {
                "filter": [{"term": {"binary_info": "binary"}}],
                "must_not": [{"has_child": {"type": "metadata", "query": {"exists": {"field": "source.name"}}}}],
            }
        }
    }
    parents = ctx.man.binary2.w.delete_loop(ctx.sd, body)

    # ensure that next delete also factors in parent deletion we just performed
    ctx.refresh()
    # delete metadata with no parent binary
    body = {
        "query": {
            "bool": {
                "filter": [{"term": {"binary_info": "metadata"}}],
                "must_not": [{"has_parent": {"parent_type": "binary", "query": {"exists": {"field": "binary_info"}}}}],
            }
        }
    }
    metadata = ctx.man.binary2.w.delete_loop(ctx.sd, body)
    return parents + metadata


class LinkReport:
    """Container for stats generated by ensure_valid_links."""

    total_deleted: int = 0


def ensure_valid_links(
    ctx: context.Context,
    sha256s: Iterable[str] | None = None,
    *,
    recursion: int = 20,
    report: LinkReport | None = None,
) -> Generator[tuple[str, str, str], None, None]:
    """This function will ensure that links to supplied parent binaries are 'consistent'.

    Consistent is defined as 'no children have links to non-existent parents'.

    It requires a list of seed sha256s of binaries that may have been deleted.
    If these binaries are no longer present, it'll delete any parent-child links where they are the parent.

    This function is not needed for ageoff as it targets submissions, which are fully connected.

    As there can be many children for the incoming hashes, this is implemented as a generator
    to prevent memory issues and to allow each sha256 to be recorded in case something fails.

    If yield_purges==true, will be much slower but yield info needed to purge binaries from s3.

    If a report dict is supplied, will be updated with information about the consistency operation.
    """
    # Theres no global constant for the max depth limit in Azul, so we pick a large recursion limit
    # which should be way more than any depth limit in the system.
    total_deleted = 0

    # abort if at limit
    if recursion <= 0:
        return

    # determine if the sha256s still exist in the system
    # FUTURE this set could be stored to disk to ensure that the operation can be retried.
    children_affected = set()
    for chunk in utils.chunker(sha256s, max_items=1000):
        # if binary no longer exists, add to next working set
        body = {
            "query": {
                "bool": {
                    "filter": [{"ids": {"values": chunk}}],
                }
            },
        }
        resp = ctx.man.binary2.w.search(ctx.sd, body)
        # any id we didn't see has been deleted
        deleted_parents = list(set(chunk).difference(x["_id"] for x in resp["hits"]["hits"]))
        # find all children with parents in the set
        body = {
            "_source": "sha256",
            "query": {
                "bool": {
                    "filter": [{"terms": {"parent.sha256": deleted_parents}}],
                }
            },
        }
        # record child sha256 for every link we are about to delete from
        for child in ctx.man.binary2.w.scan(ctx.sd, body):
            children_affected.add(child["_source"]["sha256"])
            yield child["_source"]["sha256"]

        # delete parent-child links
        # FUTURE this could be a delete by id to prevent possible issue where data enters system after
        # the above scan and is deleted without being inspected.
        total_deleted += ctx.man.binary2.w.delete_loop(ctx.sd, body=body)

    if not children_affected:
        # no child links were deleted so there is nothing more to process
        return

    # we may have just deleted the last link for a binary
    # This consistency check will then delete all directly linked docs associated with that binary
    total_deleted += ensure_valid_binaries(ctx)

    if report:
        # update report before doing recursive call
        report.total_deleted += total_deleted

    # recursively check every child that we just altered to see if it was deleted, and delete any links to it
    yield from ensure_valid_links(ctx, children_affected, recursion=recursion - 1, report=report)
